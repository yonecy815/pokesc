import os
import requests
from bs4 import BeautifulSoup
import streamlit as st
from pathlib import Path
from io import BytesIO
import zipfile
import re
from datetime import datetime, timedelta, timezone

# æ—¥æœ¬æ™‚é–“ã®å–å¾—é–¢æ•°
def get_japan_time_str():
    JST = timezone(timedelta(hours=9))
    now = datetime.now(JST)
    return now.strftime('%Y-%m-%d_%H-%M-%S')

# å•†å“åã®æŠ½å‡ºã¨ãƒ•ã‚¡ã‚¤ãƒ«åæ•´å½¢
def get_product_name(soup):
    title = soup.find("title")
    if title:
        name = title.text.split('|')[0].strip()
        name = re.sub(r'[\\/*?:"<>|]', "", name)
        return name
    return "å•†å“åä¸æ˜"

# ç”»åƒURLã¨ãƒ•ã‚©ãƒ«ãƒ€åã®å–å¾—
def scrape_images_and_name(url):
    headers = {
        "User-Agent": "Mozilla/5.0"
    }
    response = requests.get(url, headers=headers)
    soup = BeautifulSoup(response.content, 'html.parser')

    product_name = get_product_name(soup)
    date_str = get_japan_time_str()
    folder_name = f"{product_name}_{date_str}"

    photo_list_div = soup.find('div', class_='photoList')
    image_urls = []

    if photo_list_div:
        item_divs = photo_list_div.find_all('div', class_='item')
        for item in item_divs:
            first_img = item.find('img')
            if first_img:
                img_url = first_img.get("src")
                if img_url and not img_url.startswith("http"):
                    img_url = requests.compat.urljoin(url, img_url)
                if img_url and 'data:image' not in img_url:
                    image_urls.append(img_url)

    return folder_name, image_urls

# ZIPãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆ
def create_zip(images, folder_name):
    zip_buffer = BytesIO()
    with zipfile.ZipFile(zip_buffer, "w") as zip_file:
        for idx, img_url in enumerate(images):
            try:
                img_data = requests.get(img_url).content
                file_path = f"{folder_name}/image_{idx + 1}.jpg"
                zip_file.writestr(file_path, img_data)
            except Exception as e:
                print(f"ç”»åƒå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
                continue
    zip_buffer.seek(0)
    return zip_buffer

# -------------------------------
# Streamlit ã‚¢ãƒ—ãƒªæœ¬ä½“
# -------------------------------

correct_password = "1212"

if "authenticated" not in st.session_state:
    st.session_state.authenticated = False

if not st.session_state.authenticated:
    st.title("ç”»åƒãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ„ãƒ¼ãƒ«ğŸ”’")
    password_input = st.text_input("æš—è¨¼ç•ªå·ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„:", type="password")

    if password_input == correct_password:
        st.session_state.authenticated = True
        st.success("èªè¨¼ã«æˆåŠŸã—ã¾ã—ãŸã€‚ãƒ„ãƒ¼ãƒ«ã‚’èµ·å‹•ã—ã¾ã™ã€‚")
        st.rerun()  # â† ã“ã“ã‚’ä¿®æ­£æ¸ˆã¿
    elif password_input:
        st.error("æš—è¨¼ç•ªå·ãŒé–“é•ã£ã¦ã„ã¾ã™ã€‚")
else:
    st.title("ãƒã‚±ãƒ¢ãƒ³ã‚»ãƒ³ã‚¿ãƒ¼ç”»åƒã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¢ãƒ—ãƒª")

    url = st.text_input("å•†å“ãƒšãƒ¼ã‚¸ã®URLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„:")

    if st.button("ç”»åƒã‚’å–å¾—"):
        if url:
            with st.spinner("ç”»åƒã‚’å–å¾—ä¸­..."):
                folder_name, image_urls = scrape_images_and_name(url)
                if image_urls:
                    st.success(f"{folder_name} ã‹ã‚‰ {len(image_urls)} æšã®ç”»åƒã‚’å–å¾—ã—ã¾ã—ãŸã€‚")
                    for img_url in image_urls:
                        st.image(img_url, width=200)
                    
                    zip_file = create_zip(image_urls, folder_name)
                    st.download_button(
                        label="ç”»åƒã‚’ã¾ã¨ã‚ã¦ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆZIPï¼‰",
                        data=zip_file,
                        file_name=f"{folder_name}.zip",
                        mime="application/zip"
                    )
                else:
                    st.warning("ç”»åƒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        else:
            st.error("URLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
